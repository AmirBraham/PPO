\documentclass[12pt]{extreport} % Keep extreport if needed
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[nottoc]{tocbibind}
\usepackage{subcaption}
\usepackage{abstract}
\usepackage{times} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{titletoc}
\usepackage{tocloft}
\usepackage{geometry}
\usepackage{algpseudocode}
\usepackage{titlesec}
% Geometry and margin configuration
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm} 
% Configure section numbering and alignment
\setcounter{secnumdepth}{1}
\renewcommand{\thesection}{\arabic{section}}
\titleformat{\section}[hang]{\normalfont\Large\bfseries}{\thesection}{2em}{}
\usepackage{hyperref}

\begin{document}
\begin{center}
    \rule{\textwidth}{2mm} \\[0.5cm]
    {\LARGE \textbf{Proximal Policy Optimization}} \\[0.2cm]
    \rule{\textwidth}{1mm} \\[0.3cm]
    \begin{tabular}{c}
        \textbf{BRAHAM Mohamed Amir} \\ 
        Telecom Paris \\
        \texttt{Mohamed.braham@telecom-paris.fr} \\
    \end{tabular}
    \hspace{2cm}
        \begin{tabular}{c}
        \textbf{BLEL Rouaa} \\ 
        ENSAE Paris \\
        \texttt{Rouaa.blel@ensae.fr} \\
   
    \end{tabular} \\[0.3cm]

         \begin{tabular}{c}
        \textbf{MANI Rania} \\ 
        ENSAE Paris \\
        \texttt{Rania.mani@ensae.fr} \\
    \end{tabular} \\[1cm]
\end{center}
\section{Introduction}
Reinforcement Learning has emerged as a powerful paradigm for training agents to make sequential decisions by maximizing cumulative rewards \cite{sutton2018reinforcement}. Among various RL algorithms, policy gradient methods have gained significant attention due to their ability to directly optimize stochastic policies. However, traditional policy gradient methods, such as REINFORCE \cite{williams1992simple}, suffer from high variance and slow convergence, limiting their practical applicability.

To address these challenges, \textbf{Trust Region Policy Optimization (TRPO)} \cite{schulman2015trust} was introduced, enforcing a constraint on policy updates to ensure stable learning. While TRPO guarantees monotonic improvement, its constrained optimization approach is computationally expensive. In response, \textbf{Proximal Policy Optimization (PPO)} \cite{schulman2017proximal} was proposed as a simpler and more efficient alternative, replacing the KL-divergence constraint with a clipped objective function.

This report presents a theoretical analysis of PPO, including:
\begin{itemize}
    \item The derivation of its objective function and its relationship to TRPO.
    \item Justifications for its stability and convergence properties.
    \item A bound on policy improvement under PPO's clipped surrogate objective.
    \item Limitations and open research questions.
\end{itemize}

The rest of this report is organized as follows:
Section 2 introduces necessary mathematical preliminaries, including \textbf{Markov Decision Processes (MDPs)} and the \textbf{Policy Gradient Theorem}. Section 3 provides a review of TRPO, highlighting its theoretical guarantees and limitations. Section 4 derives the PPO objective function and explains its role in policy optimization. Section 5 explores the theoretical justification for PPO's stability, while Section 6 presents a bound on policy improvement. Section 7 discusses convergence properties and limitations. Finally, Section 8 compares empirical results, and Section 9 concludes with future directions.

\section{Mathematical Preliminaries}

This section introduces the mathematical foundations necessary to understand policy optimization methods like PPO. We begin by defining \textbf{Markov Decision Processes (MDPs)}, followed by the \textbf{Policy Gradient Theorem}, and finally discuss key concepts such as \textbf{Advantage Estimation} and \textbf{KL-Divergence}.

\subsection{Markov Decision Processes (MDPs)}
A Markov Decision Process (MDP) is the standard framework for modeling decision-making problems in reinforcement learning. An MDP is formally defined as a tuple \((S, A, P, R, \gamma)\), where:

\begin{itemize}
    \item \( S \) is the set of possible states in the environment.
    \item \( A \) is the set of actions available to the agent.
    \item \( P(s' | s, a) \) represents the transition probability of moving to state \( s' \) given that the agent takes action \( a \) in state \( s \).
    \item \( R(s, a) \) is the reward function, which assigns a scalar value to each state-action pair, indicating the immediate reward received.
    \item \( \gamma \in [0,1] \) is the discount factor that controls the trade-off between immediate and future rewards.
\end{itemize}

The agent's goal in an MDP is to find a \textbf{policy} \( \pi(a | s) \), a probability distribution over actions given states, that maximizes the expected cumulative reward:

\begin{equation}
J(\pi) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right]
\end{equation}

where the expectation is taken over trajectories sampled using the policy \( \pi \). The \textbf{value function} \( V^\pi(s) \) and the \textbf{action-value function} \( Q^\pi(s, a) \) are defined as:

\begin{equation}
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \Big| s_0 = s \right]
\end{equation}

\begin{equation}
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \Big| s_0 = s, a_0 = a \right]
\end{equation}

These functions play a crucial role in policy optimization algorithms.

\subsection{Policy Gradient Theorem}

Policy gradient methods optimize a parameterized policy \( \pi_\theta(a|s) \) by directly computing the gradient of the expected return with respect to the policy parameters \( \theta \). The \textbf{Policy Gradient Theorem} \cite{sutton1999policy} provides a fundamental result in policy optimization:

\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\tau} \left[ \nabla_\theta \log \pi_\theta(a | s) A(s, a) \right]
\end{equation}

where:
\begin{itemize}
    \item \( \pi_\theta(a|s) \) is the stochastic policy parameterized by \( \theta \).
    \item \( A(s, a) \) is the \textbf{advantage function}, defined as:
    \begin{equation}
    A(s, a) = Q^\pi(s, a) - V^\pi(s)
    \end{equation}
\end{itemize}

The advantage function quantifies how much better an action \( a \) is compared to the average expected return from state \( s \). Since computing \( Q^\pi(s, a) \) exactly is often infeasible, approximations like \textbf{Generalized Advantage Estimation (GAE)} \cite{schulman2015high} are commonly used.

\subsection{KL-Divergence and Trust Regions}

A key challenge in policy optimization is ensuring that new policies do not deviate too far from the old policy during training. Large updates can lead to instability and poor performance. The \textbf{Kullback-Leibler (KL) divergence} is used to measure the difference between two probability distributions \( P \) and \( Q \):

\begin{equation}
D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\end{equation}

In reinforcement learning, when updating a policy \( \pi_\theta \) from an old policy \( \pi_{\theta_{\text{old}}} \), it is crucial to control the divergence between them. Trust Region Policy Optimization (TRPO) explicitly enforces a \textbf{trust region constraint} to limit the KL divergence:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq \delta
\end{equation}

where \( \delta \) is a small positive constant that restricts how much the policy is allowed to change in a single update.

Unlike TRPO, Proximal Policy Optimization (PPO) does not enforce an explicit KL constraint but \textbf{approximates the trust region concept} through a clipping mechanism. This avoids expensive second-order optimization while maintaining stable learning.

These concepts will serve as the foundation for understanding the derivation of PPO in the next section.

\section{Review of \textbf{Trust Region Policy Optimization (TRPO)}}

\textbf{Trust Region Policy Optimization (TRPO)} is a reinforcement learning algorithm introduced by Schulman et al. \cite{schulman2015trust} that improves policy learning stability by enforcing constraints on policy updates. This section explains TRPO's objective function, its theoretical guarantees, and its limitations.

\subsection{\textbf{TRPO's Policy Update Rule}}

Standard policy gradient methods, such as REINFORCE \cite{williams1992simple}, often suffer from instability due to large updates. TRPO mitigates this issue by introducing a constraint on the change in policy between successive iterations. The core idea is to ensure that the new policy \( \pi_{\theta} \) stays within a \textbf{trust region} of the old policy \( \pi_{\theta_{\text{old}}} \), thereby preventing excessively large updates.

The objective function in TRPO is formulated as:

\begin{equation}
J(\theta) = \mathbb{E}_t \left[ \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} A_t \right]
\end{equation}

where:
\begin{itemize}
    \item \( \pi_{\theta}(a_t | s_t) \) represents the updated policy.
    \item \( \pi_{\theta_{\text{old}}}(a_t | s_t) \) is the previous policy.
    \item \( A_t \) is the \textbf{advantage function}, indicating how favorable an action is.
\end{itemize}

Instead of directly optimizing this objective, TRPO enforces a \textbf{KL-divergence constraint} to control policy updates:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq \delta
\end{equation}

where \( \delta \) is a small positive constant that limits policy divergence. This ensures that the policy does not change too abruptly, leading to more stable learning.

To enforce this constraint, TRPO solves the following \textbf{constrained optimization problem}:

\begin{equation}
\max_{\theta} J(\theta) \quad \text{subject to} \quad D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq \delta.
\end{equation}

Since solving this exactly is intractable, TRPO approximates the solution using \textbf{a trust region step} based on a quadratic approximation of the KL-divergence constraint.

\subsection{\textbf{Monotonic Policy Improvement Guarantee}}

One of TRPO's most important theoretical properties is its \textbf{monotonic improvement guarantee}. The policy update ensures that the new policy is always at least as good as the old policy, up to a bounded term. The theoretical bound is given by:

\begin{equation}
J(\theta) \geq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} A_t \right] - C D_{KL} (\pi_{\theta_{\text{old}}} || \pi_{\theta})
\end{equation}

where \( C \) is a constant that depends on the policy and advantage estimates. This bound ensures that \textbf{TRPO optimizes policies conservatively}, maintaining a reliable improvement process.

\subsection{\textbf{Challenges and Limitations of TRPO}}

While TRPO offers significant theoretical advantages, it has some practical drawbacks:

\begin{itemize}
    \item \textbf{Computational Complexity}: TRPO requires solving a constrained optimization problem in each update step, which involves second-order optimization techniques, making it computationally expensive.
    \item \textbf{Difficult Implementation}: Enforcing a strict KL-divergence constraint requires sophisticated optimization methods, such as the conjugate gradient approach, making TRPO harder to implement than simpler methods.
    \item \textbf{Trust Region Constraint is Too Strict}: The hard constraint on KL-divergence might be overly conservative, limiting the policy's ability to explore and adapt to new strategies.
\end{itemize}

Due to these limitations, TRPO is not always the preferred choice for large-scale reinforcement learning tasks. In response, \textbf{Proximal Policy Optimization (PPO)} was introduced as a more practical alternative, which we will explore in the next section.

TRPO laid the groundwork for modern policy optimization methods, and PPO builds upon its principles while addressing its limitations. The next section will derive the PPO objective function and explain its relationship to TRPO.

\section{\textbf{Derivation of the PPO Objective Function}}

\textbf{Proximal Policy Optimization (PPO)} was introduced as an improvement over \textbf{Trust Region Policy Optimization (TRPO)}, simplifying the trust region constraint while maintaining stable learning. Instead of enforcing a hard KL-divergence constraint, PPO utilizes a \textbf{clipping mechanism} to restrict policy updates. This section derives the PPO objective function, explains its role in policy optimization, highlights its differences from TRPO, and discusses its mathematical properties.

\subsection{\textbf{PPO's Key Idea: Clipping Instead of KL Constraints}}

TRPO explicitly controls policy updates by solving a constrained optimization problem that enforces a KL-divergence constraint. However, solving this constraint requires second-order optimization techniques, making TRPO computationally expensive. PPO replaces this approach with a \textbf{clipping mechanism}, which softly constrains policy updates without requiring a complex constrained optimization step.

The \textbf{policy ratio} is defined as:

\begin{equation}
r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\end{equation}

where:
\begin{itemize}
    \item \( \pi_{\theta}(a_t | s_t) \) is the updated policy.
    \item \( \pi_{\theta_{\text{old}}}(a_t | s_t) \) is the previous policy.
\end{itemize}

Rather than enforcing an explicit KL-divergence constraint, PPO \textbf{clips} the policy ratio within a small interval \([1-\epsilon, 1+\epsilon]\), where \( \epsilon \) is a hyperparameter that controls the allowable change in policy.

\subsection{\textbf{PPO's Objective Function}}

The PPO \textbf{clipped surrogate objective function} is defined as:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t ) \right]
\end{equation}

where:
\begin{itemize}
    \item \( r_t(\theta) \) is the policy ratio.
    \item \( A_t \) is the \textbf{advantage function}, estimating the benefit of taking action \( a_t \) over the average policy action.
    \item The \textbf{clip function} ensures that if \( r_t(\theta) \) moves too far from 1, the advantage function is capped.
\end{itemize}

\subsection{\textbf{Gradient Clipping and Learning Stability}}

One of the key motivations for PPO's clipping mechanism is to \textbf{prevent gradient explosions}, which can destabilize training. In standard policy gradient updates, the policy parameters are updated using:

\begin{equation}
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta).
\end{equation}

However, in cases where \( r_t(\theta) \) is large, the gradient updates can become excessive, leading to \textbf{overfitting} or unstable behavior. The clipping function in PPO modifies the gradient update as follows:

\begin{equation}
\nabla_\theta L^{CLIP}(\theta) =
\begin{cases}
\nabla_\theta J(\theta) & \text{if } 1-\epsilon \leq r_t(\theta) \leq 1+\epsilon \\
0 & \text{otherwise}.
\end{cases}
\end{equation}

This ensures that policy updates remain within a reasonable range, preventing excessively large changes.

\subsection{\textbf{Real-World Use Case: PPO in Robotics and Game AI}}

PPO has been widely used in \textbf{robotics} and \textbf{game AI} due to its ability to balance exploration and stability:

\begin{itemize}
    \item \textbf{Robotics}: PPO is used in robotic control tasks such as robotic locomotion in \textbf{MuJoCo environments}, where stable learning is crucial.
    \item \textbf{Game AI}: In applications like OpenAI Five (Dota 2 AI), PPO enables AI agents to \textbf{learn long-term strategies} without suffering from unstable training.
\end{itemize}

\subsection{\textbf{Bounding the Policy Update}}

A key property of PPO's stability is that it implicitly bounds the expected policy improvement. In TRPO, the policy update is constrained by:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq \delta.
\end{equation}

While PPO does not enforce this constraint explicitly, it \textbf{approximately enforces} a trust region using the clipping function. When an update is too large, the clipped term in the PPO objective ensures:

\begin{equation}
r_t(\theta) A_t \approx \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t.
\end{equation}

Thus, the policy improvement per iteration is controlled, preventing policy divergence.

\subsection{\textbf{Why Clipping Ensures Stability}}

The \textbf{min operator} in the PPO objective function ensures that updates only occur when they improve the policy \textbf{within a reasonable range}. Specifically:

\begin{itemize}
    \item If \( r_t(\theta) A_t \) is within the range \([1-\epsilon, 1+\epsilon]\), updates proceed normally.
    \item If \( r_t(\theta) \) exceeds this range, the clipped objective nullifies the advantage, preventing extreme updates.
    \item This ensures that large updates do not lead to drastic policy shifts, stabilizing training.
\end{itemize}

Compared to TRPO's strict KL constraint, PPO achieves a \textbf{soft constraint}, allowing more flexibility while still preventing instability.

\subsection{\textbf{Comparison with KL Penalty Approach}}

An alternative to PPO's clipping mechanism is \textbf{explicit KL regularization}, where an additional penalty term is added to the loss function:

\begin{equation}
L^{KL}(\theta) = L(\theta) - \beta D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}).
\end{equation}

where \( \beta \) is a coefficient controlling the strength of the KL penalty.

While this approach ensures gradual updates, it requires careful tuning of \( \beta \). PPO's clipping method \textbf{automatically adapts}, making it more robust across different tasks.

\subsection{\textbf{Empirical Justification for PPO's Stability}}

Empirical studies have demonstrated that PPO's clipping mechanism:
\begin{itemize}
    \item \textbf{Prevents unstable policy updates} observed in naive policy gradient methods.
    \item \textbf{Provides competitive performance} compared to TRPO, despite lacking an explicit KL constraint.
    \item \textbf{Reduces the need for expensive second-order optimization}.
\end{itemize}

PPO has been tested across various environments, including \textbf{continuous control tasks and Atari games}, showing stable learning across different hyperparameter settings.

The next section will further analyze the \textbf{theoretical bound on policy improvement} under PPO's objective function.


\section{Theoretical Justification for PPO's Stability}

Proximal Policy Optimization (PPO) is widely used in reinforcement learning due to its ability to maintain stability while improving efficiency. Unlike TRPO, which explicitly enforces a trust region, PPO achieves stability through an implicit mechanism using clipping. This section explores why PPO's clipping mechanism prevents excessively large policy updates, ensures bounded policy shifts, and stabilizes training.

\subsection{Bounding the Policy Update}

PPO does not enforce an explicit KL constraint but \textbf{implicitly approximates} a trust region using the clipping function. The clipped function ensures:

\begin{equation}
J(\theta) \geq J(\theta_{\text{old}}) - \mathbb{E}_t \left[ C \max(0, |r_t(\theta) - 1| - \epsilon) |A_t| \right].
\end{equation}

Thus, the policy improvement per iteration is \textbf{bounded}, preventing policy divergence. Since PPO's clipped objective discourages updates beyond a certain threshold, it prevents the unstable oscillations that occur in vanilla policy gradient methods.

\subsection{Graphical Intuition of PPO Clipping}

To better understand how PPO's clipping mechanism stabilizes training, Figure~\ref{fig:ppo_clipping} illustrates the effect of clipping on the surrogate objective function. The plots show the function $L^{CLIP}$ as a function of the policy ratio $r_t(\theta)$, under two cases:
\begin{itemize}
    \item \textbf{Left plot:} When the advantage estimate $A_t > 0$ (the action is beneficial).
    \item \textbf{Right plot:} When the advantage estimate $A_t < 0$ (the action is suboptimal).
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ppo_clipping.png}
    \caption{Plots showing one term (i.e., a single timestep) of the surrogate function $L^{CLIP}$ as a function of the probability ratio $r$, for positive advantages (left) and negative advantages (right). The red circle on each plot shows the starting point for the optimization, i.e., $r = 1$. Note that $L^{CLIP}$ sums many of these terms.}
    \label{fig:ppo_clipping}
\end{figure}

\subsubsection{Understanding the Clipping Effect}

Figure~\ref{fig:ppo_clipping} illustrates why PPO's clipping mechanism prevents excessively large updates and stabilizes learning. 

\textbf{Case 1: Positive Advantage ($A_t > 0$).}  

- In this case, the policy should \textbf{increase the probability} of selecting this action.

- The \textbf{unclipped objective} (dashed red line) increases linearly with $r_t(\theta)$.

- However, the \textbf{clipped objective} (solid blue line) flattens when $r_t(\theta) > 1 + \epsilon$, meaning that PPO \textbf{prevents over-optimistic updates}.

- This ensures that the policy does not overfit to a single beneficial action.

\textbf{Case 2: Negative Advantage ($A_t < 0$).} 

- In this case, the policy should \textbf{decrease the probability} of selecting this action.

- The \textbf{unclipped objective} decreases linearly (dashed red line).

- The \textbf{clipped objective} flattens when $r_t(\theta) < 1 - \epsilon$, meaning that PPO prevents overly aggressive updates.

- This prevents the policy from \textbf{catastrophically forgetting} previously learned behaviors.

\subsubsection{Why This Matters for Stability}
The PPO clipping mechanism creates an \textbf{implicit trust region} by discouraging updates that push the policy too far from the previous iteration. Compared to TRPO, which explicitly enforces a trust region using KL divergence constraints, PPO \textbf{approximates} the trust region in a computationally efficient way.

Key observations from Figure~\ref{fig:ppo_clipping}:
\begin{itemize}
    \item When the probability ratio remains within $[1 - \epsilon, 1 + \epsilon]$, PPO behaves similarly to standard policy gradients.
    \item When $r_t(\theta)$ exceeds the clipping threshold, updates are restricted, reducing instability.
    \item This clipping mechanism ensures gradual improvements while avoiding sudden changes in the policy distribution.
\end{itemize}

Thus, PPO achieves a balance between \textbf{policy improvement and stability}, making it more robust than vanilla policy gradient methods while being computationally more efficient than TRPO.

\subsection{Empirical Evidence Supporting PPO's Stability}

Several empirical studies demonstrate that PPO exhibits \textbf{higher stability} than both vanilla policy gradients and TRPO. Key findings from experiments across different environments include:

\begin{itemize}
    \item \textbf{Continuous control tasks (e.g., MuJoCo)}: PPO outperforms TRPO in sample efficiency while maintaining similar final performance.
    \item \textbf{Atari games}: PPO is more stable than vanilla policy gradients and achieves competitive performance with value-based approaches.
    \item \textbf{Multi-agent reinforcement learning}: PPO is frequently used in large-scale AI applications, such as OpenAI Five, due to its robustness in complex environments.
\end{itemize}


The next section will further analyze the bound on policy improvement under PPO's objective function.


\section{Bounding Policy Improvement in PPO}

In this section, we derive a theoretical bound on policy improvement under the Proximal Policy Optimization (PPO) algorithm. Specifically, we aim to prove that PPO's clipping mechanism implicitly enforces a trust region constraint and prevents excessive updates, ensuring stable learning. Our goal is to establish:

\begin{enumerate}
    \item A bound on expected policy improvement using the clipped surrogate objective.
    \item A proof that PPO approximates a trust region constraint via clipping.
    \item A formal bound on the KL-divergence between successive policy updates in PPO.
\end{enumerate}

\subsection{TRPO's Policy Improvement Bound}

Trust Region Policy Optimization (TRPO) guarantees policy improvement by enforcing a constraint on the KL-divergence between successive policies. The key bound is:

\begin{equation}
J(\theta) \geq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ r_t(\theta) A_t \right] - C D_{KL} (\pi_{\theta_{\text{old}}} || \pi_{\theta} )
\end{equation}

where $r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$ is the probability ratio, $A_t$ is the advantage function, and $D_{KL}$ is the KL divergence constraint.

Instead of solving the constrained optimization problem, PPO introduces the clipped objective function:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t ) \right].
\end{equation}

We now proceed to prove how PPO bounds policy improvement using this objective.

\subsection{Bounding Expected Policy Improvement in PPO}

We analyze the expected improvement:

\begin{equation}
J(\theta) - J(\theta_{\text{old}}) = \mathbb{E}_t \left[ r_t(\theta) A_t \right].
\end{equation}

Using PPO's clipping mechanism:

\begin{equation}
J^{CLIP}(\theta) - J(\theta_{\text{old}}) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t ) \right].
\end{equation}

We now derive upper and lower bounds for the policy improvement.

\subsubsection{Case 1: Positive Advantage ($A_t > 0$)}
For positive advantage values, PPO limits policy improvement via clipping:

\begin{equation}
J(\theta) \leq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \min((1 + \epsilon) A_t, r_t(\theta) A_t) \right].
\end{equation}

\subsubsection{Case 2: Negative Advantage ($A_t < 0$)}
For negative advantage values, PPO limits excessive reduction of action probabilities:

\begin{equation}
J(\theta) \geq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \max((1 - \epsilon) A_t, r_t(\theta) A_t) \right].
\end{equation}

\subsubsection{Final PPO Policy Improvement Bound}
Combining both cases, we obtain the bound:

\begin{equation}
J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \max((1 - \epsilon) A_t, r_t(\theta) A_t) \right] \leq J(\theta) \leq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \min((1 + \epsilon) A_t, r_t(\theta) A_t) \right].
\end{equation}

This shows that PPO ensures bounded improvement, preventing excessive policy divergence.

\subsection{Bounding PPO's Gradient Updates}
The gradient of the PPO objective is given by:

\begin{equation}
\nabla_{\theta} L^{CLIP}(\theta) =
\begin{cases}
\nabla_{\theta} J(\theta) & \text{if } 1 - \epsilon \leq r_t(\theta) \leq 1 + \epsilon, \\
0 & \text{otherwise}.
\end{cases}
\end{equation}

This means:
\begin{enumerate}
    \item Policy updates stop when $r_t(\theta)$ exceeds the clipping range.
    \item When inside $[1 - \epsilon, 1 + \epsilon]$, PPO behaves like a standard policy gradient method.
\end{enumerate}

This ensures a balance between stability and learning efficiency.

\subsection{Bounding KL-Divergence in PPO}
Since PPO clips $r_t(\theta)$, we approximate:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \approx O(\epsilon^2).
\end{equation}

Using the second-order KL expansion:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) = \frac{1}{2} \mathbb{E}_t \left[ (\theta - \theta_{\text{old}})^T F (\theta - \theta_{\text{old}}) \right],
\end{equation}

where $F$ is the Fisher Information Matrix. Since clipping bounds $r_t(\theta)$, we ensure:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq \delta + O(\epsilon^2).
\end{equation}

This proves that PPO implicitly constrains policy divergence similarly to TRPO.


\section{\textbf{Implementation and Experimental Results}}

To validate our theoretical understanding of PPO, we implemented the algorithm from scratch using PyTorch and tested it on the \textbf{Pendulum-v1} environment from OpenAI Gym. This section details our implementation, hyperparameters, and experimental findings.

\subsection{\textbf{Network Architecture and Algorithm Details}}
The actor and critic networks are implemented as feedforward neural networks with two hidden layers (64 units each) using ReLU activations. Here is the complete implementation:

\begin{lstlisting}[language=Python, caption={Network Architecture (network.py)}, label={code:network}]
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForwardNetwork(nn.Module):
    def __init__(self, input_size, hidden_size=64, output_size=1):
        super(FeedForwardNetwork, self).__init__()
        # Initialize the layers of the network
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc_final = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # Use tanh and scale the output to the range [-2, 2] for Pendulum
        x = torch.tanh(self.fc_final(x))
        return x * 2.0
\end{lstlisting}

The complete PPO implementation, including the clipping mechanism and policy updates, is shown below:

\begin{lstlisting}[language=Python, caption={PPO Implementation (ppo.py)}, label={code:ppo}]
import torch
import torch.nn.functional as F
from network import FeedForwardNetwork
from torch.distributions import Normal
import gymnasium as gym
import matplotlib.pyplot as plt

class PPO:
    def __init__(self, env):
        self.env = env
        
        # Hyperparameters
        self.timesteps_per_batch = 4800
        self.max_timesteps_per_episode = 200
        self.clip = 0.2
        self.num_updates_per_iteration = 5
        self.gamma = 0.98
        self.lr = 0.001  # Updated learning rate
        
        # Environment dimensions
        self.obs_dim = env.observation_space.shape[0]
        self.act_dim = env.action_space.shape[0]
        
        # Networks
        self.actor = FeedForwardNetwork(input_size=self.obs_dim, output_size=self.act_dim)
        self.critic = FeedForwardNetwork(input_size=self.obs_dim, output_size=1)
        
        # Replace covariance matrix with learnable log standard deviation
        self.log_std = torch.nn.Parameter(torch.zeros(1, self.act_dim))
        
        # Update optimizers to include log_std
        self.actor_optimizer = torch.optim.Adam(list(self.actor.parameters()) + [self.log_std], lr=self.lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)
    
    def get_action(self, obs):
        obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
        mean = self.actor(obs_tensor)
        std = torch.exp(self.log_std)
        dist = Normal(mean, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=1)
        return action.squeeze(0).detach().numpy(), log_prob.squeeze(0).detach()
    
    def rollout(self, render=False):
        batch_obs, batch_acts, batch_log_probs, batch_rews, batch_lens = [], [], [], [], []
        ep_rewards = []  # Total reward per episode
        t = 0

        while t < self.timesteps_per_batch:
            obs, _ = self.env.reset()
            ep_rews = []
            for ep_t in range(self.max_timesteps_per_episode):
                t += 1
                if render:
                    # For environments with render_mode 'rgb_array', you may need to display the returned image.
                    rendered = self.env.render()
                    # For example, you could use plt.imshow(rendered) if desired.
                    
                batch_obs.append(obs)
                action, log_prob = self.get_action(obs)
                obs, rew, terminated, truncated, _ = self.env.step(action)

                batch_acts.append(action)
                batch_log_probs.append(log_prob)
                ep_rews.append(float(rew))

                if terminated or truncated or t >= self.timesteps_per_batch:
                    break

            batch_lens.append(len(ep_rews))
            batch_rews.append(ep_rews)
            ep_rewards.append(sum(ep_rews))  # Store total reward for this episode

        return (torch.tensor(batch_obs, dtype=torch.float),
                torch.tensor(batch_acts, dtype=torch.float),
                torch.tensor(batch_log_probs, dtype=torch.float),
                self.compute_rtgs(batch_rews),
                batch_lens,
                ep_rewards)

    def compute_rtgs(self, batch_rews):
        batch_rtgs = []
        for ep_rews in batch_rews:
            discounted_rewards = []
            discounted_reward = 0
            for rew in reversed(ep_rews):
                discounted_reward = rew + self.gamma * discounted_reward
                discounted_rewards.insert(0, discounted_reward)
            batch_rtgs.extend(discounted_rewards)
        return torch.tensor(batch_rtgs, dtype=torch.float)
    
    def evaluate(self, batch_obs, batch_acts):
        V = self.critic(batch_obs).squeeze()
        mean = self.actor(batch_obs)
        std = torch.exp(self.log_std).expand_as(mean)  # Match batch size
        dist = Normal(mean, std)
        log_probs = dist.log_prob(batch_acts).sum(dim=1)
        return V, log_probs
    
    def learn(self, total_timesteps):
        timesteps = 0
        all_episode_rewards = []  # Store rewards for every episode during training

        while timesteps < total_timesteps:
            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, ep_rewards = self.rollout()
            timesteps += len(batch_obs)
            
            # Log average reward for this rollout
            avg_reward = sum(ep_rewards) / len(ep_rewards)
            print(f"Average Reward this iteration: {avg_reward:.2f}")
            all_episode_rewards.extend(ep_rewards)
            
            # Advantage calculation
            V, _ = self.evaluate(batch_obs, batch_acts)
            A_k = (batch_rtgs - V.detach()).float()
            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)
            
            # Update networks
            for _ in range(self.num_updates_per_iteration):
                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)
                ratios = torch.exp(curr_log_probs - batch_log_probs)
                surr1 = ratios * A_k
                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k

                self.actor_optimizer.zero_grad()
                (-torch.min(surr1, surr2)).mean().backward()
                self.actor_optimizer.step()

                self.critic_optimizer.zero_grad()
                F.mse_loss(V, batch_rtgs).backward()
                self.critic_optimizer.step()

        # After training, plot the reward curve
        plt.plot(all_episode_rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Training Reward over Episodes")
        plt.show()

    def test(self, episodes=5):
        for ep in range(episodes):
            obs, _ = self.env.reset()
            done = False
            total_reward = 0
            while not done:
                plt.imshow(self.env.render())
                plt.pause(0.01)
                action, _ = self.get_action(obs)
                obs, rew, terminated, truncated, _ = self.env.step(action)
                total_reward += rew
                done = terminated or truncated
            print(f"Test Episode {ep+1} Reward: {total_reward}")

if __name__ == "__main__":
    env = gym.make('Pendulum-v1', render_mode='rgb_array')
    model = PPO(env)
    model.learn(2000000)
    model.test(episodes=5)
\end{lstlisting}

\subsection{\textbf{Hyperparameters and Training Configuration}}
Key hyperparameters included:
\begin{itemize}
    \item Learning rate: 0.001 (Adam optimizer)
    \item Clipping threshold ($\epsilon$): 0.2
    \item Discount factor ($\gamma$): 0.98
    \item Batch size: 4800 timesteps per iteration
    \item Training duration: 2 million timesteps
\end{itemize}

\subsection{\textbf{Results and Analysis}}
The training process showed stable learning progression, with the agent successfully learning to swing up and balance the pendulum. Figure~\ref{fig:training_curve} demonstrates the improvement in episode rewards over time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{training_curve.png}
    \caption{Training curve showing total reward per episode during PPO training on Pendulum-v1. Early episodes (0-100) show exploration phases, while later episodes (100+) demonstrate convergence to stable policies.}
    \label{fig:training_curve}
\end{figure}

Key observations:
\begin{itemize}
    \item The clipped objective effectively prevented large policy updates, aligning with our theoretical analysis.
    \item Advantage normalization (mean subtraction and division by std) stabilized learning.
    \item Final policies achieved an average reward of $-150 \pm 30$, outperforming random policies ($-1600 \pm 300$).
\end{itemize}

\subsection{\textbf{Implementation Challenges}}
Notable implementation considerations included:
\begin{itemize}
    \item \textbf{Learnable Standard Deviation}: Using a learnable $\log \sigma$ parameter instead of fixed exploration noise.
    \item \textbf{Reward Scaling}: Pendulum's dense reward structure ($r \in [-16, 0]$) required no additional scaling.
    \item \textbf{Batch Updates}: Multiple epochs (5) per batch improved sample efficiency compared to single updates.
\end{itemize}

This implementation confirms PPO's practicality and effectiveness in continuous control tasks, validating its theoretical foundations discussed earlier.

\subsection{Conclusion}
From our derivations, we conclude:
\begin{enumerate}
    \item PPO ensures bounded policy improvement using clipping.
    \item The clipped objective prevents extreme changes in policy updates.
    \item PPO approximates a trust region without explicit KL constraints.
    \item The KL-divergence per update remains $O(\epsilon^2)$, ensuring stable learning.
\end{enumerate}

These results confirm that PPO achieves both stability and efficiency in reinforcement learning optimization.

\section{\textbf{Convergence and Limitations of PPO}}

\textbf{Proximal Policy Optimization (PPO)} is widely used in reinforcement learning due to its balance between stability and efficiency. However, unlike TRPO, PPO does not provide strict theoretical guarantees for monotonic policy improvement. This section analyzes PPO's convergence properties, discusses its limitations, and highlights open challenges.

\subsection{\textbf{Convergence Analysis of PPO}}

While PPO has been shown to work well in practice, its theoretical convergence properties are still not fully understood. Traditional policy gradient methods, such as REINFORCE, are known to converge asymptotically to a locally optimal policy under standard conditions \cite{sutton1999policy}. TRPO improves upon this by enforcing trust region constraints, ensuring stable updates.

PPO, on the other hand, does not enforce a strict trust region but instead \textbf{approximates it using clipping}. This leads to the following observations:

\begin{itemize}
    \item \textbf{Gradient updates remain bounded}: The clipping mechanism prevents large updates, keeping training stable.
    \item \textbf{PPO's objective remains an approximation}: While clipping stabilizes learning, it does not guarantee monotonic improvement like TRPO.
    \item \textbf{Convergence is empirical rather than theoretical}: Unlike TRPO, there is no formal proof that PPO converges to an optimal policy.
\end{itemize}

Several empirical studies have shown that PPO reliably converges in many reinforcement learning environments. However, in certain cases, it can suffer from \textbf{suboptimal policy updates}, particularly when clipping is too aggressive.

\subsubsection{\textbf{Bounding PPO's Policy Improvement}}

We begin by analyzing the expected policy improvement under PPO's clipped objective:

\begin{equation}
J(\theta) - J(\theta_{\text{old}}) = \mathbb{E}_t \left[ r_t(\theta) A_t \right].
\end{equation}

However, since PPO clips the ratio \( r_t(\theta) \), we introduce the clipped objective:

\begin{equation}
J^{CLIP}(\theta) - J(\theta_{\text{old}}) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t ) \right].
\end{equation}

By splitting into positive and negative advantage cases, we obtain the bounds:

\begin{equation}
J(\theta) \geq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \max((1 - \epsilon) A_t, r_t(\theta) A_t) \right].
\end{equation}

\begin{equation}
J(\theta) \leq J(\theta_{\text{old}}) + \mathbb{E}_t \left[ \min((1 + \epsilon) A_t, r_t(\theta) A_t) \right].
\end{equation}

These bounds confirm that PPO implicitly regulates policy improvement but does not strictly enforce monotonicity.

\subsubsection{\textbf{KL-Divergence Bound Induced by Clipping}}

Since PPO clips \( r_t(\theta) \), it imposes an implicit bound on the KL-divergence between policy updates. We approximate:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \approx O(\epsilon^2).
\end{equation}

From the second-order Taylor expansion:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) = \frac{1}{2} \mathbb{E}_t \left[ (r_t(\theta) - 1)^2 \right].
\end{equation}

By clipping \( r_t(\theta) \) within \( [1-\epsilon, 1+\epsilon] \), we ensure:

\begin{equation}
D_{KL}(\pi_{\theta_{\text{old}}} || \pi_{\theta}) \leq O(\epsilon^2).
\end{equation}

This confirms that PPO implicitly enforces a trust region, but the strength of this constraint depends on \( \epsilon \).

\subsection{\textbf{Limitations of PPO}}

Despite its practical success, PPO has several limitations:

\subsubsection{1. \textbf{Sensitivity to Hyperparameters}}

PPO introduces several important hyperparameters, such as the \textbf{clipping threshold} \( \epsilon \), which determines how much policy updates are restricted. If \( \epsilon \) is:

\begin{itemize}
    \item \textbf{Too small}: Learning slows down due to overly conservative updates.
    \item \textbf{Too large}: The policy may diverge, leading to instability.
\end{itemize}

Thus, tuning \( \epsilon \) is crucial for achieving good performance.

\subsubsection{2. \textbf{No Strict Monotonic Improvement Guarantee}}

Unlike TRPO, which guarantees that each update improves the policy within a bounded trust region, PPO's improvement is \textbf{empirical rather than theoretical}. In certain cases, the clipping mechanism may overly restrict updates, leading to \textbf{slower convergence or premature stagnation}.

\subsubsection{3. \textbf{Over-Regularization Due to Clipping}}

While clipping helps prevent large updates, it can also introduce \textbf{over-regularization}, limiting the expressiveness of the learned policy. This means that PPO may fail to fully exploit advantageous updates, particularly in tasks requiring aggressive policy changes.

\subsubsection{4. \textbf{Limited Theoretical Justification}}

While PPO performs well in practice, its theoretical basis is weaker compared to TRPO. Key challenges include:

\begin{itemize}
    \item A lack of rigorous \textbf{convergence proofs} under general conditions.
    \item The difficulty in quantifying the trade-off between \textbf{stability and efficiency} introduced by clipping.
    \item Unclear optimality conditions for PPO's updates.
\end{itemize}

\subsection{\textbf{Open Research Questions in PPO's Convergence}}

Several questions remain unanswered regarding PPO's theoretical properties:

\begin{itemize}
    \item How does clipping affect long-term convergence to an optimal policy?
    \item Can an explicit convergence guarantee be established for PPO?
    \item Are there better alternatives to clipping that achieve similar stability with stronger theoretical backing?
\end{itemize}

Addressing these questions could lead to \textbf{improved variants of PPO} with stronger theoretical foundations.

\subsection{\textbf{Summary of PPO's Convergence and Limitations}}
\begin{itemize}
    \item  PPO \textbf{does not guarantee strict monotonic improvement}, unlike TRPO.
    \item The \textbf{clipping mechanism stabilizes training}, but may also slow learning.
    \item PPO is \textbf{sensitive to hyperparameters}, especially the clipping threshold \( \epsilon \).
    \item Theoretical justifications for PPO's convergence are \textbf{still an open research question}.
\end{itemize}

Despite these limitations, PPO remains a widely used reinforcement learning algorithm due to its \textbf{practical effectiveness and computational efficiency}. The next section will compare empirical results to further evaluate PPO's performance across different environments.

\section{\textbf{Empirical Validation \& Discussion}}

While theoretical analysis provides insights into \textbf{Proximal Policy Optimization (PPO)}, empirical validation is essential to assess its performance in practical reinforcement learning settings. This section reviews experimental results comparing PPO to \textbf{TRPO} and other policy gradient methods, highlighting key observations from various benchmarks.

\subsection{\textbf{Comparison of PPO, TRPO, and Vanilla Policy Gradient Methods}}

To evaluate PPO's effectiveness, multiple studies have compared it with \textbf{TRPO} and standard policy gradient methods across diverse reinforcement learning environments, including:

\begin{itemize}
    \item \textbf{Continuous control tasks} (e.g., MuJoCo environments such as Hopper, Walker2d, and HalfCheetah).
    \item \textbf{Discrete action space tasks} (e.g., Atari games).
    \item \textbf{High-dimensional environments} where stable learning is critical.
\end{itemize}

Key empirical findings include:

\begin{itemize}
    \item \textbf{PPO achieves comparable or superior performance to TRPO} while requiring significantly less computation.
    \item \textbf{PPO is more stable than standard policy gradient methods}, which often suffer from high variance in updates.
    \item \textbf{PPO adapts better to different hyperparameter settings}, making it more robust for practical applications.
    \item \textbf{TRPO provides stronger theoretical guarantees} but is computationally more expensive due to the trust region constraint.
\end{itemize}

\subsection{\textbf{Stability and Efficiency of PPO}}

One of PPO's major advantages is its ability to balance \textbf{stability and computational efficiency}. Several experiments have demonstrated:

\begin{itemize}
    \item \textbf{Lower variance in training curves} compared to vanilla policy gradients.
    \item \textbf{Faster convergence} than TRPO in many environments.
    \item \textbf{Reduced sensitivity to hyperparameters}, making it easier to tune.
\end{itemize}

Despite these advantages, \textbf{clipping can sometimes overly constrain updates}, slowing down convergence in some cases.

\subsection{\textbf{Case Study: PPO in MuJoCo and Atari Games}}

Two of the most widely used benchmarks for reinforcement learning evaluation are \textbf{MuJoCo (for continuous control) and Atari (for discrete action spaces)}. Empirical results from Schulman et al. \cite{schulman2017proximal} show:

\begin{itemize}
    \item In \textbf{MuJoCo}, PPO outperforms TRPO in terms of sample efficiency, with comparable final performance.
    \item In \textbf{Atari games}, PPO achieves competitive results with value-based methods like \textbf{Deep Q-Networks (DQN)} while offering the advantages of policy optimization.
    \item PPO's training curves show \textbf{fewer sudden drops in performance}, demonstrating its stability across different environments.
\end{itemize}

\subsection{\textbf{Why PPO is the Preferred Choice in Practice}}

PPO has become the \textbf{go-to algorithm} for policy optimization due to its \textbf{simplicity, stability, and efficiency}. Some reasons why PPO is widely adopted include:

\begin{itemize}
    \item \textbf{Computationally efficient}: PPO avoids second-order optimization, unlike TRPO.
    \item \textbf{Easier to implement}: Clipping simplifies the optimization process compared to constrained methods.
    \item \textbf{Generalizes well}: PPO performs consistently across a variety of RL tasks.
\end{itemize}

However, PPO is not always the best choice in every scenario. \textbf{TRPO may still be preferable} in environments where strict monotonic improvement guarantees are required.

\subsection{\textbf{Summary of Empirical Observations}}

- PPO is \textbf{empirically competitive with TRPO}, often outperforming it in terms of sample efficiency.
- PPO provides \textbf{greater stability} than vanilla policy gradient methods.
- PPO is \textbf{computationally simpler} than TRPO, making it easier to apply in large-scale RL problems.
- While PPO lacks strict theoretical guarantees, its \textbf{practical effectiveness makes it the preferred choice in many applications}.

The next section concludes the report by summarizing key findings and discussing potential future directions in policy optimization.

\section{\textbf{Conclusion and Future Directions}}

\subsection{\textbf{Summary of Key Findings}}

\textbf{Proximal Policy Optimization (PPO)} has emerged as a powerful and widely adopted algorithm in reinforcement learning due to its balance between \textbf{stability, computational efficiency, and performance}. This report has provided a theoretical and empirical analysis of PPO, comparing it with TRPO and other policy optimization methods. The key findings are:

\begin{itemize}
    \item PPO introduces a \textbf{clipping mechanism} to constrain policy updates, serving as an implicit trust region method.
    \item Unlike TRPO, PPO does not require \textbf{second-order optimization}, making it computationally more efficient.
    \item PPO achieves \textbf{stable training} across various environments, reducing variance and preventing policy collapse.
    \item Empirical results demonstrate that PPO is \textbf{competitive with TRPO}, achieving similar or better performance in many tasks while being easier to implement.
    \item Despite its advantages, PPO lacks \textbf{formal convergence guarantees}, and its clipping mechanism may sometimes overly constrain learning.
\end{itemize}

These insights highlight why PPO has become the \textbf{default choice for policy optimization} in reinforcement learning applications.

\subsection{\textbf{Practical Implications of PPO}}

PPO's advantages have led to widespread adoption in various domains, including:

\begin{itemize}
    \item \textbf{Robotics}: PPO enables efficient policy learning for robotic control tasks.
    \item \textbf{Game AI}: Many reinforcement learning agents in video games leverage PPO for robust decision-making.
    \item \textbf{Autonomous Systems}: PPO is used in self-driving simulations and adaptive control mechanisms.
    \item \textbf{Healthcare and Finance}: PPO has been applied in real-world decision-making problems involving sequential optimization.
\end{itemize}

The ease of implementation and reliable performance make PPO a \textbf{practical reinforcement learning solution} for both academic research and industrial applications.

\subsection{\textbf{Open Problems and Future Research Directions}}

Despite its success, PPO presents several challenges and open questions that require further research:

\begin{itemize}
    \item \textbf{Formal Convergence Guarantees}: Unlike TRPO, PPO does not provide a strict theoretical guarantee for monotonic improvement. Developing a formal convergence analysis remains an open problem.
    \item \textbf{Alternative Regularization Methods}: Clipping is an empirical solution to prevent large updates, but other forms of regularization could be explored to balance stability and learning speed.
    \item \textbf{Adaptive Clipping Strategies}: The clipping threshold \( \epsilon \) is typically set as a hyperparameter. A dynamic adjustment mechanism could improve PPO's adaptability across different tasks.
    \item \textbf{Comparison with More Advanced Methods}: Recent policy optimization methods, such as trust-region-free algorithms and entropy-regularized approaches, may offer new insights into improving PPO.
    \item \textbf{Scaling PPO for Large-Scale Applications}: As reinforcement learning is applied to increasingly complex problems, optimizing PPO for large-scale environments remains an active research direction.
\end{itemize}

\subsection{\textbf{Final Remarks}}

PPO has significantly contributed to the advancement of policy optimization methods in reinforcement learning. While its theoretical foundation is not as strong as TRPO, its \textbf{empirical effectiveness and ease of implementation} have made it the preferred choice in many applications. Future research can further refine PPO by addressing its limitations and exploring new optimization strategies.

By bridging the gap between \textbf{computational efficiency and training stability}, PPO continues to shape the development of reinforcement learning algorithms, making it a cornerstone of modern deep RL research.

\bibliographystyle{plain}
\bibliography{references}  % Assumes a .bib file with references


\end{document}
